---
layout: post
title: "Part 2: Making the Most of Async in Node.js"
topics: [Concurrency, Development]
published: false
---

In the previous article, I very briefly described a bunch of concepts in distributed computing, but 
before we get to the meat of this series, there's an elephant in the room that I have so far 
ignored.

## The Global Interpreter Lock.

With very few exceptions, languages like Ruby or Node.js can really only do one thing at a time.
Node.js uses libuv to do some blocking system calls and parts of network requests in the background,
but everything else happens in a single thread. You cannot make a fetch while processing the 
result of another, and so there are certain sorts of latency you have to watch out for. All you have
available to you is cooperative multitasking, and if you want to do anything more performant than
some basic async code, accomplishing this without making your code look completely alien to all of
your current and future coworkers, if not also yourself, is quite challenging.

This is where libraries like p-limit or throat come in.

## p-limit

[p-limit](https://github.com/sindresorhus/p-limit) takes an async arrow function and return a promise. It can run N functions at the
same time, as Promise resolution allows, and any additional functions go into a queue. When any one
of those N functions resolves, then it starts the next function that was added to the queue. So for
the cost of an additional arrow function, you get some of the benefits of cooperative multitasking
without having to further pollute your code base.

## Additional patterns

There are, however, a number of limitations and gotchas which the following code patterns will help
you reduce or eliminate.

### Keep Your Function Short and Simple

Anything in your arrow function stops any new requests from starting. That is simple enough to 
understand. But any complex actions performed within the `limit()` call can also lead to deadlock 
situations. One common use case for generating lots of network requests in batch processing is to 
load a tree of values. When writing naive Node.js code, we often lazily evaluate these calls, so 
that we have a promise being resolved, resulting in several more requests, which in turn resolve 
and then generate kÂ² additional requests in turn. If you do this inside of a queue, eventually the
queue will fill up and then your earlier async functions can never resolve because they are gumming
up the queue. 

Instead, you should make the `limit()` function very short, and deal with any consequences of that
`resolve()` on the promise that is returned from limit. That way your initial call leaves the queue,
and can be replaced by any recursive calls that are made, either to the same endpoint or to others.

`Promise.all/for of`:

```javascript

  const limit = pLimit(10);

  const responses = entries.map(limit(async (entry) => getData(entry)));

  for (let response of responses) {
    const data = parseResponse(await response);

    // Now the response has been removed from the queue, and the next getData call can fire.
    
    const children = data.children.map(limit(async (child) => getData(child)));
    // ...
  }
```

This will prevent deadlocks, however it will now also cause the entire job to run as a breadth-
first search; all but N of the top level requests will resolve, and then the first child request
will run. Composing several instances of `p-limit` can be useful here, and is easier to see when
working with dependent requests. 

### One Queue Per Task




